# -*- coding: utf-8 -*-
"""SpeechRecognition Final Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Psdx0VkHgN5sWfVp7MdjnII46EjURods

# **Final Code**
"""

!pip install deepface
!pip install transformers
# !pip install tensorflow-gpu
!pip install git+https://github.com/linto-ai/whisper-timestamped

import gc
import torch
from deepface import DeepFace
from collections import Counter
from transformers import pipeline
from collections import defaultdict
import whisper_timestamped as whisper
from moviepy.editor import VideoFileClip
from datetime import datetime, timedelta

def getAnalysis(videoPath):

    faceEmotionTimeFrame = {}
    faceSentimentTimeFrame = {}

    # audio from video
    output_audio_path = 'test2.wav'
    videoClip = VideoFileClip(videoPath)
    audio_clip = videoClip.audio
    audio_clip.write_audiofile(output_audio_path, codec='pcm_s16le')

    # audio analysis
    audio = whisper.load_audio("test2.wav")
    model = whisper.load_model("medium", device="cuda")
    result = whisper.transcribe(model, audio, language="en", detect_disfluencies=True)
    del model

    duration = videoClip.duration
    # fps = videoClip.fps if videoClip.fps <= 30 else 15
    fps = 15

    for segment in result["segments"]:

        frames = []
        start = segment["start"]
        end = segment["end"]
        key = f"{start}:{end}"

        faceEmotionTimeFrame[key] = {}
        faceSentimentTimeFrame[key] = {}

        # Load the selected segment using MoviePy
        selectedSegment = videoClip.subclip(start, end)

        # facial expression detection
        for idx, (tstamp, frame) in enumerate(selectedSegment.iter_frames(with_times=True, fps=15)):

            try:
                res = DeepFace.analyze(frame, actions=["emotion"])
                if res[0]["dominant_emotion"] == "neutral":
                    if res[0]["emotion"][res[0]["dominant_emotion"]] > 80:
                        faceSentimentTimeFrame[key][start + round(tstamp * 1000) / 1000] = "neutral"
                        faceEmotionTimeFrame[key][start + round(tstamp * 1000) / 1000] = "neutral"
                elif res[0]["dominant_emotion"] in ["happy", "surprise"]:
                    if res[0]["emotion"][res[0]["dominant_emotion"]] > 80:
                        faceSentimentTimeFrame[key][start + round(tstamp * 1000) / 1000] = "positive"
                        faceEmotionTimeFrame[key][start + round(tstamp * 1000) / 1000] = res[0]["dominant_emotion"]
                elif res[0]["dominant_emotion"] in ["angry", "sad", "fear"]:
                    if res[0]["emotion"][res[0]["dominant_emotion"]] > 80:
                        faceSentimentTimeFrame[key][start + round(tstamp * 1000) / 1000] = "negative"
                        faceEmotionTimeFrame[key][start + round(tstamp * 1000) / 1000] = res[0]["dominant_emotion"]

            except: continue

        selectedSegment.reader.close()

    videoClip.close()
    torch.cuda.empty_cache()
    gc.collect()

    return result, faceEmotionTimeFrame, faceSentimentTimeFrame, duration

audioResult, videoEmotions, videoSentiments, duration = getAnalysis("video5.mp4") #10min

pipe = pipeline("text-classification", model="cardiffnlp/twitter-roberta-base-sentiment-latest", device="cuda")

def getFinalEmotionTimeFrame(videoTimeframe, duration):

    doHourly = 0
    hours = 0
    doMinute = 0
    minutes = 0
    lastKey = 0
    currKey = 1
    response = {}

    times = list(videoTimeframe.keys())[1:]

    def helper(currKey, lastKey):
        sentimentCounter = Counter([videoTimeframe[x] for x in times if lastKey < x <= currKey])
        if sentimentCounter: return sentimentCounter.most_common(1)[0][0]
        else: return ""

    if duration / 60 >= 1:
        doMinute = 1
        minutes = round(duration / 60)
    if doMinute and (duration / 60) / 60 >= 1:
        doHourly = 1
        hours = round(minutes // 60)

    if doHourly:

        for i in range(hours):
            response[i] = {}
            for j in range(60):
                response[i][j] = {}
                for k in range(60):
                    currKey = ( i * 3600 ) + ( j * 60 ) + k+1
                    response[i][j][k] = helper(currKey, lastKey)
                    lastKey = currKey

    elif doMinute:

        for i in range(minutes):
            response[i] = {}
            for j in range(60):
                currKey = ( i * 60 ) + j+1
                response[i][j] = helper(currKey, lastKey)
                lastKey = currKey

    else:

        for i in range(60):
            currKey = i+1
            response[i] = helper(currKey, lastKey)
            lastKey = currKey

    return response

audioEmotionTimeFrame = {}
for segment in audioResult["segments"]:
    emotion = pipe(segment["text"])[0]["label"]
    for word in segment["words"]:
        audioEmotionTimeFrame[word["start"]] = emotion

videoResult = {}

for key in videoSentiments:
    for time in videoSentiments[key]:
        videoResult[time] = videoSentiments[key][time]

videoEmotion = {}

for key in videoEmotions:
    for time in videoEmotions[key]:
        videoEmotion[time] = videoEmotions[key][time]

videoSentimentResponse = getFinalEmotionTimeFrame(videoResult, duration)
videoEmotionResponse = getFinalEmotionTimeFrame(videoEmotion, duration)
audioResponse = getFinalEmotionTimeFrame(audioEmotionTimeFrame, duration)

mismatchTimeFrames = {}

for tFrame in audioResponse:

    # if level 2 is present, can be minute or second
    if isinstance(audioResponse[tFrame], dict):

        for tFrame2 in audioResponse[tFrame]:

            # if level 3 is present, it is for second
            if isinstance(audioResponse[tFrame][tFrame2], dict):

                for tFrame3 in audioResponse[tFrame][tFrame2]:

                    # this is last level, check for audio and video
                    if audioResponse[tFrame][tFrame2][tFrame3] != "" and videoSentimentResponse[tFrame][tFrame2][tFrame3] != "":

                        if videoSentimentResponse[tFrame][tFrame2][tFrame3] != audioResponse[tFrame][tFrame2][tFrame3]:
                            print(f"at {tFrame} hour {tFrame2} minute {tFrame3} second sentiment of video of {videoEmotionResponse[tFrame][tFrame2][tFrame3]} and sentiment of audio is {audioResponse[tFrame][tFrame2][tFrame3]}")
                            key1 = tFrame if len(str(tFrame)) == 2 else '0'+ str(tFrame)
                            key2 = tFrame2 if len(str(tFrame2)) == 2 else '0'+ str(tFrame2)
                            key3 = tFrame3 if len(str(tFrame3)) == 2 else '0'+ str(tFrame3)
                            mismatchTimeFrames[f"{key1}:{key2}:{key3}"] = {"video": videoEmotionResponse[tFrame][tFrame2][tFrame3], "audio": audioResponse[tFrame][tFrame2][tFrame3]}

            # else check for audio and video
            elif audioResponse[tFrame][tFrame2] != "" and videoSentimentResponse[tFrame][tFrame2] != "":

                if videoSentimentResponse[tFrame][tFrame2] != audioResponse[tFrame][tFrame2]:
                    print(f"at {tFrame} minute {tFrame2} second sentiment of video of {videoEmotionResponse[tFrame][tFrame2]} and sentiment of audio is {audioResponse[tFrame][tFrame2]}")
                    key1 = tFrame if len(str(tFrame)) == 2 else '0'+ str(tFrame)
                    key2 = tFrame2 if len(str(tFrame2)) == 2 else '0'+ str(tFrame2)
                    mismatchTimeFrames[f"00:{key1}:{key2}"] = {"video": videoEmotionResponse[tFrame][tFrame2], "audio": audioResponse[tFrame][tFrame2]}

    # else check for audio and video
    elif audioResponse[tFrame] != "" and videoSentimentResponse[tFrame] != "":

        if videoSentimentResponse[tFrame] != audioResponse[tFrame]:
            print(f"at {tFrame} second sentiment of video of {videoEmotionResponse[tFrame]} and sentiment of audio is {audioResponse[tFrame]}")
            key1 = tFrame if len(str(tFrame)) == 2 else '0'+ str(tFrame)
            mismatchTimeFrames[f"00:00:{key1}"] = {"video": videoEmotionResponse[tFrame], "audio": audioResponse[tFrame]}

mismatchTimeFrames

from datetime import datetime

def helper(jsonData):

    timeList = list(jsonData.keys())
    result = {}


    # Convert string times to datetime objects
    timeObjects = [datetime.strptime(time_str, '%H:%M:%S') for time_str in timeList]

    # Initialize a dict to store elements with time differences <= 5 seconds
    result = {}

    # Iterate through the time objects to find elements with time differences <= 5 seconds
    for i in range(len(timeObjects)-1):
        time_diff = abs((timeObjects[i] - timeObjects[i+1]).total_seconds())
        if time_diff <= 5:
            result[timeList[i]] = jsonData[timeList[i]]

    time_diff = abs((timeObjects[-2] - timeObjects[-1]).total_seconds())
    if time_diff <= 5:
        result[timeList[-1]] = jsonData[timeList[-1]]

    # Print the elements with time differences <= 5 seconds
    if result != jsonData:
        return helper(result)

    return result

from datetime import datetime

def groupAndFilter(mismatchTimeFrames):

    # Sort the dictionary by 'audio' values and keys
    sortedData = dict(sorted(mismatchTimeFrames.items(), key=lambda x: (x[1]['audio'], x[0])))

    # Initialize dictionaries to store groups for 'neutral', 'positive', and 'negative' audio values
    neutralGroup = {}
    positiveGroup = {}
    negativeGroup = {}

    lastMergedSeconds = {"neutral": None, "positive": None, "negative": None}

    # Iterate through the sorted data to group items by 'audio' value
    for key, value in sortedData.items():

        audio_value = value['audio']
        second = int(key.split(":")[2])
        if lastMergedSeconds[audio_value] is None:
            lastMergedSeconds[audio_value] = second

        if audio_value == 'neutral':
            currentGroup = neutralGroup
        elif audio_value == 'positive':
            currentGroup = positiveGroup
        elif audio_value == 'negative':
            currentGroup = negativeGroup
        currentGroup[key] = value



    def helper(jsonData):
        result = {}
        timeList = list(jsonData.keys())

        if len(timeList) < 2:
            return jsonData

        timeObjects = [datetime.strptime(time, '%H:%M:%S') for time in timeList]

        for i in range(len(timeObjects) - 1):
            time_diff = abs((timeObjects[i] - timeObjects[i + 1]).total_seconds())
            if time_diff <= 5:
                result[timeList[i]] = jsonData[timeList[i]]

        time_diff = abs((timeObjects[-2] - timeObjects[-1]).total_seconds())
        if time_diff <= 5:
            result[timeList[-1]] = jsonData[timeList[-1]]

        return result

    if neutralGroup and len(neutralGroup) > 2:
        neutralGroup = helper(neutralGroup)

    if positiveGroup and len(positiveGroup) > 2:
        positiveGroup = helper(positiveGroup)

    if negativeGroup and len(negativeGroup) > 2:
        negativeGroup = helper(negativeGroup)

    # Create a dictionary to store filtered groups
    filteredGroups = dict()

    # Filter and store groups with length > 8
    for group in [(neutralGroup, 'neutral'), (positiveGroup, 'positive'), (negativeGroup, 'negative')]:
        if len(group[0]) >= 5:
            filteredGroups[group[1]] = group[0]

    return filteredGroups

# Example usage:
filtered_groups = groupAndFilter(mismatchTimeFrames)
print(filtered_groups)

# time_strings = ['00:00:5', '00:00:6', '00:00:8', '00:00:9', '00:00:14']
def getTimeFrames(timeFrames):
    # Define the time format for parsing
        timeFormat = "%H:%M:%S"

        # Get a list of keys (time strings) from the timeFrames dictionary
        timeFrameKeys = [key for key in timeFrames.keys()]

        # Convert the time strings to datetime.time objects
        timeObjects = [datetime.strptime(time_str, timeFormat).time() for time_str in timeFrameKeys]

        # Initialize an empty dictionary to store time ranges
        timeRanges = {}

        # Initialize variables for the current start and end time
        currentStartTime = timeFrameKeys[0]
        currentEndTime = timeFrameKeys[0]

        def getMostCommonSentiments(sentiments):
            count = Counter(sentiments)
            common = []
            for senti in count.most_common(2):
                common.append(senti[0])

            return common

        # Initialize a list to store sentiments for video and audio
        sentiments = [[timeFrames[currentStartTime]["video"]], [timeFrames[currentStartTime]["audio"]]]

        # Iterate through the time objects
        for i in range(0, len(timeObjects) - 1):
            # Calculate the time difference in seconds between two consecutive times
            diff_seconds = ((timeObjects[i + 1].hour * 60 + timeObjects[i + 1].minute) * 60 + timeObjects[i + 1].second) - ((timeObjects[i].hour * 60 + timeObjects[i].minute) * 60 + timeObjects[i].second)

            if diff_seconds < 5:
                # Extend the current time range if the time difference is less than 5 seconds
                currentEndTime = timeFrameKeys[i + 1]
                if timeFrames[currentEndTime]["video"] != "":
                    sentiments[0].append(timeFrames[currentEndTime]["video"])
                if timeFrames[currentEndTime]["audio"] != "":
                    sentiments[1].append(timeFrames[currentEndTime]["audio"])
            else:
                if currentStartTime == currentEndTime:
                    # If the start and end times are the same, move to the next time
                    currentStartTime = timeFrameKeys[i + 1]
                    currentEndTime = timeFrameKeys[i + 1]
                    sentiments = [[timeFrames[currentStartTime]["video"]], [timeFrames[currentStartTime]["audio"]]]
                    continue

                # Convert the current start and end times to datetime objects
                startTime = datetime.strptime(currentStartTime, timeFormat)
                endTime = datetime.strptime(currentEndTime, timeFormat)

                # Check if the time range is greater than 5 seconds
                if (endTime - startTime).seconds > 5:
                    # Add the time range and sentiments to the dictionary
                    timeRanges[f"{currentStartTime} - {currentEndTime}"] = {"video": getMostCommonSentiments(sentiments[0]), "audio": list(set(sentiments[1]))}

                # Move to the next time range
                currentStartTime = timeFrameKeys[i + 1]
                currentEndTime = timeFrameKeys[i + 1]
                sentiments = [[timeFrames[currentStartTime]["video"]], [timeFrames[currentStartTime]["audio"]]]

        # Adding the last time range after the loop
        if currentStartTime != currentEndTime:
            # Convert the start and end times to datetime objects
            startTime = datetime.strptime(currentStartTime, timeFormat)
            endTime = datetime.strptime(timeFrameKeys[-1], timeFormat)

            # Check if the last time range is greater than or equal to 5 seconds
            if (endTime - startTime).seconds >= 5:
                # Add the last time range and sentiments to the dictionary
                timeRanges[f"{currentStartTime} - {timeFrameKeys[-1]}"] = {"video": getMostCommonSentiments(sentiments[0]), "audio": list(set(sentiments[1]))}

        return timeRanges

for sentiment in filtered_groups:
    print(getTimeFrames(filtered_groups[sentiment]))

# audioResult
# audioResponse
# audioEmotionTimeFrame

# videoResult
# videoEmotion
# videoEmotions
# videoSentiments
# videoEmotionResponse
# videoSentimentResponse

# mismatchTimeFrames

# filtered_groups

from json import dumps
with open("audioResult.json", "w") as f:
    f.writelines(dumps(audioResult, indent=4))
f.close()

with open("audioResponse.json", "w") as f:
    f.writelines(dumps(audioResponse, indent=4))
f.close()

with open("audioEmotionTimeFrame.json", "w") as f:
    f.writelines(dumps(audioEmotionTimeFrame, indent=4))
f.close()

with open("videoResult.json", "w") as f:
    f.writelines(dumps(videoResult, indent=4))
f.close()

with open("videoEmotion.json", "w") as f:
    f.writelines(dumps(videoEmotion, indent=4))
f.close()

with open("videoEmotions.json", "w") as f:
    f.writelines(dumps(videoEmotions, indent=4))
f.close()

with open("videoSentiments.json", "w") as f:
    f.writelines(dumps(videoSentiments, indent=4))
f.close()

with open("videoEmotionResponse.json", "w") as f:
    f.writelines(dumps(videoEmotionResponse, indent=4))
f.close()

with open("videoSentimentResponse.json", "w") as f:
    f.writelines(dumps(videoSentimentResponse, indent=4))
f.close()

with open("mismatch.json", "w") as f:
    f.writelines(dumps(mismatchTimeFrames, indent=4))
f.close()

with open("filtered_group.json", "w") as f:
    f.writelines(dumps(filtered_groups, indent=4))
f.close()

data = {
    '00:0:3': {'video': 'sad', 'audio': 'positive'},
    '00:0:7': {'video': 'sad', 'audio': 'positive'},
    '00:0:15': {'video': 'angry', 'audio': 'positive'},
    '00:0:17': {'video': 'fear', 'audio': 'positive'},
    '00:0:18': {'video': 'fear', 'audio': 'positive'},
    '00:0:19': {'video': 'neutral', 'audio': 'positive'},
    '00:0:21': {'video': 'sad', 'audio': 'neutral'},
    '00:0:22': {'video': 'sad', 'audio': 'neutral'},
    '00:0:23': {'video': 'fear', 'audio': 'neutral'},
    '00:0:24': {'video': 'angry', 'audio': 'neutral'},
    '00:0:26': {'video': 'angry', 'audio': 'neutral'},
    '00:0:27': {'video': 'sad', 'audio': 'neutral'},
    '00:0:28': {'video': 'fear', 'audio': 'neutral'},
    '00:0:29': {'video': 'happy', 'audio': 'neutral'},
    '00:0:30': {'video': 'sad', 'audio': 'neutral'},
    '00:0:31': {'video': 'fear', 'audio': 'neutral'},
    '00:0:32': {'video': 'sad', 'audio': 'neutral'},
    '00:0:33': {'video': 'sad', 'audio': 'neutral'},
    '00:0:34': {'video': 'sad', 'audio': 'neutral'},
    '00:0:35': {'video': 'sad', 'audio': 'neutral'},
    '00:0:38': {'video': 'sad', 'audio': 'neutral'},
    '00:0:39': {'video': 'sad', 'audio': 'neutral'},
    '00:0:40': {'video': 'sad', 'audio': 'neutral'},
    '00:0:41': {'video': 'sad', 'audio': 'neutral'},
    '00:0:42': {'video': 'sad', 'audio': 'neutral'},
    '00:0:44': {'video': 'happy', 'audio': 'neutral'},
    '00:0:45': {'video': 'sad', 'audio': 'neutral'},
    '00:0:46': {'video': 'surprise', 'audio': 'neutral'},
    '00:0:48': {'video': 'neutral', 'audio': 'negative'},
    '00:0:57': {'video': 'happy', 'audio': 'neutral'}
}

# Sort the dictionary items by 'video' values
sorted_data = dict(sorted(data.items(), key=lambda item: item[1]['video']))

# Initialize empty groups for each category
groups = {
    'angry': [],
    'fear': [],
    'sad': [],
    'happy': [],
    'surprise': [],
}

current_group = None
lastMergedSeconds = {"angry": None, "sad": None, "fear": None, "happy": None, "surprise": None}


# Group values based on 'video' category
for key, value in sorted_data.items():
    video_category = value['video']
    if video_category in groups:
        if current_group is None or current_group != video_category:
            current_group = video_category
        if groups[video_category]:
            second = int(key.split(":")[2])

            if abs(lastMergedSeconds[video_category] - second) <= 2:
                groups[video_category][-1].append((key, value))
                lastMergedSeconds[video_category] = int(key.split(":")[2])
            else:
                if len(groups[video_category][-1]) < 5:
                    groups[video_category][-1] = [(key, value)]
                else:
                    groups[video_category].append([(key, value)])
                lastMergedSeconds[video_category] = int(key.split(":")[2])
        else:
            groups[video_category].append([(key, value)])
            lastMergedSeconds[video_category] = int(key.split(":")[2])

    else:
        current_group = None


final_groups = {}
# Print the grouped data
for category, group_values in groups.items():
    if group_values:
        final_groups[category] = []
        for group in group_values:
            if len(group) >= 5:
                doc = {}
                for key, value in group:
                    doc[key] = value
                final_groups[category].append(doc)

final_groups

def create_grouped_dict(data):
    # Sort the dictionary items by 'video' values
    sorted_data = dict(sorted(data.items(), key=lambda item: item[1]['video']))

    # Initialize empty groups for each category
    groups = {
        'angry': [],
        'fear': [],
        'sad': [],
        'happy': [],
        'surprise': [],
        'neutral': []
    }

    current_group = None
    lastMergedSeconds = {"angry": None, "sad": None, "fear": None, "happy": None, "surprise": None}

    # Group values based on 'video' category
    for key, value in sorted_data.items():
        video_category = value['video']
        if video_category in groups:
            if current_group is None or current_group != video_category:
                current_group = video_category
            if groups[video_category]:
                second = int(key.split(":")[2])
                if abs(lastMergedSeconds[video_category] - second) <= 3:
                    groups[video_category][-1].append((key, value))
                    lastMergedSeconds[video_category] = int(key.split(":")[2])
                else:
                    groups[video_category][-1] = [(key, value)]
                    lastMergedSeconds[video_category] = int(key.split(":")[2])
            else:
                groups[video_category].append([(key, value)])
                lastMergedSeconds[video_category] = int(key.split(":")[2])

        else:
            current_group = None

    # print(groups)
    # Create a dictionary of groups with length > 5
    final_groups = {}
    # Print the grouped data
    for category, group_values in groups.items():
        if group_values:
            final_groups[category] = []
            for group in group_values:
                if len(group) >= 5:
                    doc = {}
                    for key, value in group:
                        doc[key] = value
                    final_groups[category].append(doc)

    return final_groups

# Example usage:
# data = {
#     # Your data dictionary here
# }

grouped_dict = create_grouped_dict(mismatchTimeFrames)
grouped_dict
