# -*- coding: utf-8 -*-
"""SpeechRecognition Final Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Psdx0VkHgN5sWfVp7MdjnII46EjURods

# **Final Code**
"""

# !pip install deepface
# !pip install transformers
# # !pip install tensorflow-gpu
# !pip install git+https://github.com/linto-ai/whisper-timestamped

import gc
import torch
from deepface import DeepFace
from collections import Counter
from transformers import pipeline
from collections import defaultdict
import whisper_timestamped as whisper
from moviepy.editor import VideoFileClip
from datetime import datetime

def getAnalysis(videoPath):

    faceEmotionTimeFrame = {}
    faceSentimentTimeFrame = {}

    # audio from video
    output_audio_path = 'test.wav'
    videoClip = VideoFileClip(videoPath)
    audio_clip = videoClip.audio
    audio_clip.write_audiofile(output_audio_path, codec='pcm_s16le')

    # audio analysis
    audio = whisper.load_audio("test.wav")
    model = whisper.load_model("medium", device="cuda")
    result = whisper.transcribe(model, audio, language="en", detect_disfluencies=True)
    del model

    duration = videoClip.duration
    # fps = videoClip.fps if videoClip.fps <= 30 else 15
    fps = 15

    for segment in result["segments"]:

        frames = []
        start = segment["start"]
        end = segment["end"]
        key = f"{start}:{end}"

        faceEmotionTimeFrame[key] = {}
        faceSentimentTimeFrame[key] = {}

        # Load the selected segment using MoviePy
        selectedSegment = videoClip.subclip(start, end)

        # facial expression detection
        for idx, (tstamp, frame) in enumerate(selectedSegment.iter_frames(with_times=True, fps=15)):

            try:
                res = DeepFace.analyze(frame, actions=["emotion"])
                emotion = res[0]["dominant_emotion"]
                round_tstamp = start + round(tstamp * 1000) / 1000

                if emotion == "neutral":
                    if res[0]["emotion"][emotion] > 80:
                        faceSentimentTimeFrame[key][round_tstamp] = "neutral"
                        faceEmotionTimeFrame[key][round_tstamp] = emotion

                elif emotion in ["happy", "surprise"]:
                    if res[0]["emotion"][emotion] > 80:
                        faceSentimentTimeFrame[key][round_tstamp] = "positive"
                        faceEmotionTimeFrame[key][round_tstamp] = emotion

                elif emotion in ["angry", "sad", "fear"]:
                    if res[0]["emotion"][emotion] > 80:
                        faceSentimentTimeFrame[key][round_tstamp] = "negative"
                        faceEmotionTimeFrame[key][round_tstamp] = emotion

            except: continue

        selectedSegment.reader.close()

    videoClip.close()
    torch.cuda.empty_cache()
    gc.collect()

    return result, faceEmotionTimeFrame, faceSentimentTimeFrame, duration

audioResult, videoEmotions, videoSentiments, duration = getAnalysis("video5.mkv") #10min

pipe = pipeline("text-classification", model="cardiffnlp/twitter-roberta-base-sentiment-latest", device="cuda")

def getFinalEmotionTimeFrame(videoTimeframe, duration):

    hours = 0
    minutes = 0
    lastKey = 0
    currKey = 1
    response = {}

    times = list(videoTimeframe.keys())[1:]

    def helper(currKey, lastKey):
        sentimentCounter = Counter([videoTimeframe[x] for x in times if lastKey < x <= currKey])
        if sentimentCounter: return sentimentCounter.most_common(1)[0][0]
        else: return ""

    if duration / 60 >= 1:
        minutes = round(duration / 60)
        for i in range(minutes):
            response[min] = {j: helper((j * 60) + j + 1, (i - 1) * 60 + 60 + j) for j in range(60)}

    elif (duration / 60) / 60 >= 1:
        hours = round(minutes // 60)
        for i in range(hours):
            response[i] = {j : { k : helper((i * 3600) + (j * 60) + k + 1, lastKey := (i * 3600) + (j * 60) + k + 1) for k in range(60)} for j in range(60)}
    else:
        for i in range(60):
            currKey = i+1
            response[i] = helper(currKey, lastKey)
            lastKey = currKey

    return response

audioEmotionTimeFrame = {}
for segment in audioResult["segments"]:
    emotion = pipe(segment["text"])[0]["label"] # emotion value - POSITIVE, NEUTRAL, NEGATIVE
    audioEmotionTimeFrame = { word["start"] : emotion for word in segment["words"]}

videoResult = {}

#TODO - check code again
for key in videoSentiments:
    videoResult = {time : videoSentiments[key][time] for time in videoSentiments[key]}

videoEmotion = {}

for key in videoEmotions:
    videoEmotion = {time : videoEmotions[key][time] for time in videoEmotions[key]}

videoSentimentResponse = getFinalEmotionTimeFrame(videoResult, duration)
videoEmotionResponse = getFinalEmotionTimeFrame(videoEmotion, duration)
audioResponse = getFinalEmotionTimeFrame(audioEmotionTimeFrame, duration)

mismatchTimeFrames = {}

def set_mismatch_time_frames(path, audio_response, video_sentiment_response,video_emotion_response):
    if audio_response != "" and video_sentiment_response != "" and audio_response != video_sentiment_response :
        mismatchTimeFrames[path] = {"video": video_emotion_response, "audio": audio_response}

def check_tstamp(audio_response, video_sentiment_response,video_emotion_response, path=""):
    
    for key, value in audio_response.items():
        path = f"{path}:{key:02}" if path else f"{key:02}"

        if isinstance(value, dict):
            check_tstamp(audio_response=value, video_sentiment_response=video_sentiment_response[key], video_emotion_response=video_emotion_response[key], path=path)
        else:
            set_mismatch_time_frames(path=value, audio_response=value, video_sentiment_response=video_sentiment_response[key],video_emotion_response=video_emotion_response[key])

def helper(jsonData):

    timeList = list(jsonData.keys())
    result = {}


    # Convert string times to datetime objects
    timeObjects = [datetime.strptime(time_str, '%H:%M:%S') for time_str in timeList]

    # Initialize a dict to store elements with time differences <= 5 seconds
    result = {}

    # Iterate through the time objects to find elements with time differences <= 5 seconds
    for i in range(len(timeObjects)-1):
        time_diff = abs((timeObjects[i] - timeObjects[i+1]).total_seconds())
        if time_diff <= 5:
            result[timeList[i]] = jsonData[timeList[i]]

    time_diff = abs((timeObjects[-2] - timeObjects[-1]).total_seconds())
    if time_diff <= 5:
        result[timeList[-1]] = jsonData[timeList[-1]]

    # Print the elements with time differences <= 5 seconds
    if result != jsonData:
        return helper(result)

    return result


def groupAndFilter(mismatchTimeFrames):

    # Sort the dictionary by 'audio' values and keys
    sortedData = dict(sorted(mismatchTimeFrames.items(), key=lambda x: (x[1]['audio'], x[0])))

    # Initialize dictionaries to store groups for 'neutral', 'positive', and 'negative' audio values
    groups = {'neutral': {}, 'positive': {}, 'negative': {}}

    # Iterate through the sorted data to group items by 'audio' value
    for key, value in sortedData.items():

        audio_value = value['audio']
        group = groups.get(audio_value, {})
        group[key] = value

    neutralGroup = groups.get('neutral', {})
    positiveGroup = groups.get('positive', {})
    negativeGroup = groups.get('negative', {})


    def helper(jsonData):
        result = {}
        timeList = list(jsonData.keys())

        if len(timeList) < 2:
            return jsonData

        timeObjects = [datetime.strptime(time, '%H:%M:%S') for time in timeList]

        for i in range(len(timeObjects) - 1):
            time_diff = abs((timeObjects[i] - timeObjects[i + 1]).total_seconds())
            if time_diff <= 5:
                result[timeList[i]] = jsonData[timeList[i]]

        time_diff = abs((timeObjects[-2] - timeObjects[-1]).total_seconds())
        if time_diff <= 5:
            result[timeList[-1]] = jsonData[timeList[-1]]

        return result

    neutralGroup = helper(neutralGroup)

    positiveGroup = helper(positiveGroup)

    negativeGroup = helper(negativeGroup)

    # Create a dictionary to store filtered groups
    filteredGroups = dict()

    # Filter and store groups with length > 8
    filteredGroups = {group[1] : group[0] for group in [(neutralGroup, 'neutral'), (positiveGroup, 'positive'), (negativeGroup, 'negative')] if len(group[0]) >= 5}

    return filteredGroups

# Example usage:
filtered_groups = groupAndFilter(mismatchTimeFrames)
print(filtered_groups)

# time_strings = ['00:00:5', '00:00:6', '00:00:8', '00:00:9', '00:00:14']
def getTimeFrames(timeFrames):
    # Define the time format for parsing
        timeFormat = "%H:%M:%S"

        # Get a list of keys (time strings) from the timeFrames dictionary
        timeFrameKeys = [key for key in timeFrames.keys()]

        # Convert the time strings to datetime.time objects
        timeObjects = [datetime.strptime(time_str, timeFormat).time() for time_str in timeFrameKeys]

        # Initialize an empty dictionary to store time ranges
        timeRanges = {}

        # Initialize variables for the current start and end time
        currentStartTime = timeFrameKeys[0]
        currentEndTime = timeFrameKeys[0]

        def getMostCommonSentiments(sentiments):
            count = Counter(sentiments)
            common = []
            for senti in count.most_common(2):
                common.append(senti[0])

            return common

        # Initialize a list to store sentiments for video and audio
        sentiments = [[timeFrames[currentStartTime]["video"]], [timeFrames[currentStartTime]["audio"]]]

        # Iterate through the time objects
        for i in range(0, len(timeObjects) - 1):
            # Calculate the time difference in seconds between two consecutive times
            diff_seconds = ((timeObjects[i + 1].hour * 60 + timeObjects[i + 1].minute) * 60 + timeObjects[i + 1].second) - ((timeObjects[i].hour * 60 + timeObjects[i].minute) * 60 + timeObjects[i].second)

            if diff_seconds < 5:
                # Extend the current time range if the time difference is less than 5 seconds
                currentEndTime = timeFrameKeys[i + 1]
                if timeFrames[currentEndTime]["video"] != "":
                    sentiments[0].append(timeFrames[currentEndTime]["video"])
                if timeFrames[currentEndTime]["audio"] != "":
                    sentiments[1].append(timeFrames[currentEndTime]["audio"])
            else:
                if currentStartTime == currentEndTime:
                    # If the start and end times are the same, move to the next time
                    currentStartTime = timeFrameKeys[i + 1]
                    currentEndTime = timeFrameKeys[i + 1]
                    sentiments = [[timeFrames[currentStartTime]["video"]], [timeFrames[currentStartTime]["audio"]]]
                    continue

                # Convert the current start and end times to datetime objects
                startTime = datetime.strptime(currentStartTime, timeFormat)
                endTime = datetime.strptime(currentEndTime, timeFormat)

                # Check if the time range is greater than 5 seconds
                if (endTime - startTime).seconds > 5:
                    # Add the time range and sentiments to the dictionary
                    timeRanges[f"{currentStartTime} - {currentEndTime}"] = {"video": getMostCommonSentiments(sentiments[0]), "audio": list(set(sentiments[1]))}

                # Move to the next time range
                currentStartTime = timeFrameKeys[i + 1]
                currentEndTime = timeFrameKeys[i + 1]
                sentiments = [[timeFrames[currentStartTime]["video"]], [timeFrames[currentStartTime]["audio"]]]

        # Adding the last time range after the loop
        if currentStartTime != currentEndTime:
            # Convert the start and end times to datetime objects
            startTime = datetime.strptime(currentStartTime, timeFormat)
            endTime = datetime.strptime(timeFrameKeys[-1], timeFormat)

            # Check if the last time range is greater than or equal to 5 seconds
            if (endTime - startTime).seconds >= 5:
                # Add the last time range and sentiments to the dictionary
                timeRanges[f"{currentStartTime} - {timeFrameKeys[-1]}"] = {"video": getMostCommonSentiments(sentiments[0]), "audio": list(set(sentiments[1]))}

        return timeRanges

for sentiment in filtered_groups:
    print(getTimeFrames(filtered_groups[sentiment]))

# audioResult
# audioResponse
# audioEmotionTimeFrame

# videoResult
# videoEmotion
# videoEmotions
# videoSentiments
# videoEmotionResponse
# videoSentimentResponse

# mismatchTimeFrames

# filtered_groups

from json import dumps
with open("audioResult.json", "w") as f:
    f.writelines(dumps(audioResult, indent=4))
f.close()

with open("audioResponse.json", "w") as f:
    f.writelines(dumps(audioResponse, indent=4))
f.close()

with open("audioEmotionTimeFrame.json", "w") as f:
    f.writelines(dumps(audioEmotionTimeFrame, indent=4))
f.close()

with open("videoResult.json", "w") as f:
    f.writelines(dumps(videoResult, indent=4))
f.close()

with open("videoEmotion.json", "w") as f:
    f.writelines(dumps(videoEmotion, indent=4))
f.close()

with open("videoEmotions.json", "w") as f:
    f.writelines(dumps(videoEmotions, indent=4))
f.close()

with open("videoSentiments.json", "w") as f:
    f.writelines(dumps(videoSentiments, indent=4))
f.close()

with open("videoEmotionResponse.json", "w") as f:
    f.writelines(dumps(videoEmotionResponse, indent=4))
f.close()

with open("videoSentimentResponse.json", "w") as f:
    f.writelines(dumps(videoSentimentResponse, indent=4))
f.close()

with open("mismatch.json", "w") as f:
    f.writelines(dumps(mismatchTimeFrames, indent=4))
f.close()

with open("filtered_group.json", "w") as f:
    f.writelines(dumps(filtered_groups, indent=4))
f.close()

data = {
    '00:0:3': {'video': 'sad', 'audio': 'positive'},
    '00:0:7': {'video': 'sad', 'audio': 'positive'},
    '00:0:15': {'video': 'angry', 'audio': 'positive'},
    '00:0:17': {'video': 'fear', 'audio': 'positive'},
    '00:0:18': {'video': 'fear', 'audio': 'positive'},
    '00:0:19': {'video': 'neutral', 'audio': 'positive'},
    '00:0:21': {'video': 'sad', 'audio': 'neutral'},
    '00:0:22': {'video': 'sad', 'audio': 'neutral'},
    '00:0:23': {'video': 'fear', 'audio': 'neutral'},
    '00:0:24': {'video': 'angry', 'audio': 'neutral'},
    '00:0:26': {'video': 'angry', 'audio': 'neutral'},
    '00:0:27': {'video': 'sad', 'audio': 'neutral'},
    '00:0:28': {'video': 'fear', 'audio': 'neutral'},
    '00:0:29': {'video': 'happy', 'audio': 'neutral'},
    '00:0:30': {'video': 'sad', 'audio': 'neutral'},
    '00:0:31': {'video': 'fear', 'audio': 'neutral'},
    '00:0:32': {'video': 'sad', 'audio': 'neutral'},
    '00:0:33': {'video': 'sad', 'audio': 'neutral'},
    '00:0:34': {'video': 'sad', 'audio': 'neutral'},
    '00:0:35': {'video': 'sad', 'audio': 'neutral'},
    '00:0:38': {'video': 'sad', 'audio': 'neutral'},
    '00:0:39': {'video': 'sad', 'audio': 'neutral'},
    '00:0:40': {'video': 'sad', 'audio': 'neutral'},
    '00:0:41': {'video': 'sad', 'audio': 'neutral'},
    '00:0:42': {'video': 'sad', 'audio': 'neutral'},
    '00:0:44': {'video': 'happy', 'audio': 'neutral'},
    '00:0:45': {'video': 'sad', 'audio': 'neutral'},
    '00:0:46': {'video': 'surprise', 'audio': 'neutral'},
    '00:0:48': {'video': 'neutral', 'audio': 'negative'},
    '00:0:57': {'video': 'happy', 'audio': 'neutral'}
}

# Sort the dictionary items by 'video' values
sorted_data = dict(sorted(data.items(), key=lambda item: item[1]['video']))

# Initialize empty groups for each category
groups = {
    'angry': [],
    'fear': [],
    'sad': [],
    'happy': [],
    'surprise': [],
}

current_group = None
lastMergedSeconds = {"angry": None, "sad": None, "fear": None, "happy": None, "surprise": None}


# Group values based on 'video' category
for key, value in sorted_data.items():
    video_category = value['video']
    if video_category in groups:
        if groups[video_category]:
            second = int(key.split(":")[2])

            if abs(lastMergedSeconds[video_category] - second) <= 2:
                groups[video_category][-1].append((key, value))
                lastMergedSeconds[video_category] = int(key.split(":")[2])
            elif len(groups[video_category][-1]) < 5:
                    groups[video_category][-1] = [(key, value)]
            else:
                groups[video_category].append([(key, value)])
                lastMergedSeconds[video_category] = int(key.split(":")[2])
        else:
            groups[video_category].append([(key, value)])
            lastMergedSeconds[video_category] = int(key.split(":")[2])

final_groups = {}
# Print the grouped data
for category, group_values in groups.items():
    if group_values:
        final_groups[category] = []
        for group in group_values:
            if len(group) >= 5:
                final_groups[category].append({key:value for key, value in group})

final_groups

def create_grouped_dict(data):
    # Sort the dictionary items by 'video' values
    sorted_data = dict(sorted(data.items(), key=lambda item: item[1]['video']))

    # Initialize empty groups for each category
    groups = {
        'angry': [],
        'fear': [],
        'sad': [],
        'happy': [],
        'surprise': [],
        'neutral': []
    }

    current_group = None
    lastMergedSeconds = {"angry": None, "sad": None, "fear": None, "happy": None, "surprise": None}

    # Group values based on 'video' category
    for key, value in sorted_data.items():
        video_category = value['video']
        if video_category in groups:
            if groups[video_category]:
                second = int(key.split(":")[2])
                if abs(lastMergedSeconds[video_category] - second) <= 3:
                    groups[video_category][-1].append((key, value))
                    lastMergedSeconds[video_category] = int(key.split(":")[2])
                else:
                    groups[video_category][-1] = [(key, value)]
                    lastMergedSeconds[video_category] = int(key.split(":")[2])
            else:
                groups[video_category].append([(key, value)])
                lastMergedSeconds[video_category] = int(key.split(":")[2])

    # print(groups)
    # Create a dictionary of groups with length > 5
    final_groups = {}
    # Print the grouped data
    for category, group_values in groups.items():
        if group_values:
            final_groups[category] = []
            for group in group_values:
                if len(group) >= 5:
                    final_groups[category].append({key:value for key, value in group})


    return final_groups

# Example usage:
# data = {
#     # Your data dictionary here
# }

grouped_dict = create_grouped_dict(mismatchTimeFrames)
grouped_dict